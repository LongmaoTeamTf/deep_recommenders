{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a578eb21eb1f0ffbce406cca18fb76957b43a943c95f6e1cf142c9c0a29ae48e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 双塔模型\n",
    "\n",
    "Sampling-bias-corrected neural modeling for large corpus item recommendations\n",
    "\n",
    "https://dl.acm.org/doi/abs/10.1145/3298689.3346996\n",
    "\n",
    "此实验完全参照 Tensorflow_recommenders/docs/examples/deep_recommenders.ipynb"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.dont_write_bytecode = True\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from deep_recommenders import models as dprs_models\n",
    "from deep_recommenders import tasks as dprs_tasks\n",
    "from deep_recommenders import metrics as dprs_metrics"
   ]
  },
  {
   "source": [
    "## 数据准备与处理\n",
    "\n",
    "### Movielens/100k-ratings\n",
    "- user_id\n",
    "- movie_title\n",
    "- timestamp\n",
    "### Movielens/100k-movies\n",
    "- movie_title\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")\n",
    "\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"timestamp\": x[\"timestamp\"],\n",
    "})\n",
    "movies = movies.map(lambda x: x[\"movie_title\"])\n",
    "\n",
    "timestamps = np.concatenate(list(ratings.map(lambda x: x[\"timestamp\"]).batch(100)))\n",
    "\n",
    "max_timestamp = timestamps.max()\n",
    "min_timestamp = timestamps.min()\n",
    "\n",
    "timestamp_buckets = np.linspace(\n",
    "    min_timestamp, max_timestamp, num=1000,\n",
    ")\n",
    "\n",
    "unique_movie_titles = np.unique(np.concatenate(list(movies.batch(1000))))\n",
    "unique_user_ids = np.unique(np.concatenate(list(ratings.batch(1_000).map(\n",
    "    lambda x: x[\"user_id\"]))))"
   ]
  },
  {
   "source": [
    "## 模型构建\n",
    "\n",
    "### 用户模型\n",
    "\n",
    "处理用户特征，并生成embedding。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "    \"\"\"User Model\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.user_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "                vocabulary=unique_user_ids, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),\n",
    "        ])\n",
    "        self.timestamp_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.Discretization(timestamp_buckets.tolist()),\n",
    "            tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32),\n",
    "        ])\n",
    "        self.normalized_timestamp = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "\n",
    "        self.normalized_timestamp.adapt(timestamps)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # 输入为字典类型\n",
    "        return tf.concat([\n",
    "            self.user_embedding(inputs[\"user_id\"]),\n",
    "            self.timestamp_embedding(inputs[\"timestamp\"]),\n",
    "            self.normalized_timestamp(inputs[\"timestamp\"]),\n",
    "        ], axis=1)"
   ]
  },
  {
   "source": [
    "### Query塔\n",
    "\n",
    "将UserModel生成的embedding，输入多层dnn网络进行特征交叉，生成Query向量。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "\n",
    "        # We first use the user model for generating embeddings.\n",
    "        self.embedding_model = UserModel()\n",
    "\n",
    "        # Then construct the layers.\n",
    "        self.dense_layers = tf.keras.Sequential()\n",
    "\n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
    "\n",
    "        # No activation for the last layer.\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        feature_embedding = self.embedding_model(inputs)\n",
    "        return self.dense_layers(feature_embedding)"
   ]
  },
  {
   "source": [
    "### Movie模型\n",
    "\n",
    "处理Movie特征，并生成embedding。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieModel(tf.keras.Model):\n",
    "  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        max_tokens = 10_000\n",
    "\n",
    "        self.title_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "                vocabulary=unique_movie_titles, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_movie_titles) + 1, 32)\n",
    "        ])\n",
    "\n",
    "        self.title_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "            max_tokens=max_tokens)\n",
    "\n",
    "        self.title_text_embedding = tf.keras.Sequential([\n",
    "            self.title_vectorizer,\n",
    "            tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        ])\n",
    "\n",
    "        self.title_vectorizer.adapt(movies)\n",
    "\n",
    "    def call(self, titles):\n",
    "        return tf.concat([\n",
    "            self.title_embedding(titles),\n",
    "            self.title_text_embedding(titles),\n",
    "        ], axis=1)"
   ]
  },
  {
   "source": [
    "### Candidate模型\n",
    "\n",
    "将MovieModel生成的embedding，输入多层dnn网络进行特征交叉，生成Candidate向量。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CandidateModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_model = MovieModel()\n",
    "\n",
    "        # Then construct the layers.\n",
    "        self.dense_layers = tf.keras.Sequential()\n",
    "\n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
    "\n",
    "        # No activation for the last layer.\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        feature_embedding = self.embedding_model(inputs)\n",
    "        return self.dense_layers(feature_embedding)"
   ]
  },
  {
   "source": [
    "### Movielens检索模型\n",
    "\n",
    "将QueryModel与CandidateModel进行联合训练。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovielensModel(dprs_models.Model):\n",
    "\n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "        self.query_model = QueryModel(layer_sizes)\n",
    "        self.candidate_model = CandidateModel(layer_sizes)\n",
    "        self.task = dprs_tasks.Retrieval(\n",
    "            metrics=dprs_metrics.FactorizedTopK(\n",
    "                candidates=movies.batch(128).map(self.candidate_model),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        query_embeddings = self.query_model({\n",
    "            \"user_id\": features[\"user_id\"],\n",
    "            \"timestamp\": features[\"timestamp\"],\n",
    "        })\n",
    "        movie_embeddings = self.candidate_model(features[\"movie_title\"])\n",
    "\n",
    "        return self.task(\n",
    "            query_embeddings, movie_embeddings, compute_metrics=not training)"
   ]
  },
  {
   "source": [
    "## 数据准备\n",
    "\n",
    "将数据集分割为训练集和测试集。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(2048)\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "source": [
    "## 模型训练\n",
    "\n",
    "优化器采用Adagrad，学习率0.1。\n",
    "\n",
    "### 训练单层dnn的Movielens检索模型\n",
    "\n",
    "Embedding => 32"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "\n",
    "model = MovielensModel([32])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "one_layer_history = model.fit(\n",
    "    cached_train,\n",
    "    validation_data=cached_test,\n",
    "    validation_freq=5,\n",
    "    epochs=num_epochs,\n",
    "    verbose=0)\n",
    "\n",
    "accuracy = one_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
    "print(f\"Top-100 accuracy: {accuracy:.2f}.\")\n",
    "\n",
    "# Top-100 accuracy = 0.27"
   ]
  },
  {
   "source": [
    "### 训练2层dnn的Movielens检索模型\n",
    "\n",
    "\bEmbedding => 64 => Relu => 32"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MovielensModel([64, 32])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "two_layer_history = model.fit(\n",
    "    cached_train,\n",
    "    validation_data=cached_test,\n",
    "    validation_freq=5,\n",
    "    epochs=num_epochs,\n",
    "    verbose=0)\n",
    "\n",
    "accuracy = two_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
    "print(f\"Top-100 accuracy: {accuracy:.2f}.\")\n",
    "\n",
    "# Top-100 accuracy = 0.28"
   ]
  },
  {
   "source": [
    "### 训练3层dnn的Movielens检索模型\n",
    "\n",
    "Embedding => 128 => Relu => 64 => Relu => 32"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MovielensModel([128, 64, 32])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "three_layer_history = model.fit(\n",
    "    cached_train,\n",
    "    validation_data=cached_test,\n",
    "    validation_freq=5,\n",
    "    epochs=num_epochs,\n",
    "    verbose=0)\n",
    "\n",
    "accuracy = three_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
    "print(f\"Top-100 accuracy: {accuracy:.2f}.\")\n",
    "\n",
    "# Top-100 accuracy = 0.22"
   ]
  },
  {
   "source": [
    "## 训练可视化"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png_export_dir = os.path.join(\"assets\", \"deep_retrieval\")\n",
    "\n",
    "if not os.path.exists(png_export_dir):\n",
    "    os.makedirs(png_export_dir)\n",
    "\n",
    "for k in [1, 5, 10, 50, 100]:\n",
    "    num_validation_runs = len(two_layer_history.history[f\"val_factorized_top_k/top_{k}_categorical_accuracy\"])\n",
    "    epochs = [(x + 1)* 5 for x in range(num_validation_runs)]\n",
    "\n",
    "    plt.plot(epochs, one_layer_history.history[f\"val_factorized_top_k/top_{k}_categorical_accuracy\"], label=\"1 layer\")\n",
    "    plt.plot(epochs, two_layer_history.history[f\"val_factorized_top_k/top_{k}_categorical_accuracy\"], label=\"2 layers\")\n",
    "    plt.plot(epochs, three_layer_history.history[f\"val_factorized_top_k/top_{k}_categorical_accuracy\"], label=\"3 layers\")\n",
    "    plt.title(\"Accuracy vs epoch\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(f\"Top-{k} accuracy\");\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(png_export_dir, f\"top_{k}_accuracy.png\"))\n",
    "    plt.show()"
   ]
  }
 ]
}